{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf6a168-2888-4806-92b9-4b7126316547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component\n",
    "import json\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        ClassificationMetrics, \n",
    "                        component,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Metrics,\n",
    "                        Model,\n",
    "                        Output\n",
    "                       )\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1328043-d31f-4adf-b312-7a8ce363f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Configured Values\n",
    "Val = !gcloud config list --format 'value(core.project)'\n",
    "PROJECT_ID = Val[0]\n",
    "print(f\"PROJECT_ID - {PROJECT_ID}\")\n",
    "REGION = \"asia-east2\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "SERVICE_ACCOUNT = \"terraform-sa@master-314712.iam.gserviceaccount.com\"\n",
    "BUCKET_NAME = \"gs://master_asia_east_2/Vertex_AI/Sentiment_Analysis\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/Pipeline_Root/Starter\"\n",
    "print(f\"PIPELINE_ROOT - {PIPELINE_ROOT}\")\n",
    "PIPELINE_JSON_FILE = \"Starter_Analysis.json\"\n",
    "PIPELINE_EXPERIMENT_NAME = \"Starter_Scoring_Pipeline\"+TIMESTAMP\n",
    "MODEL_DISPLAY_NAME = \"Sentiment_analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a629f-0617-4078-87bc-4fb35ed4a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d74a9f4-964c-4ddd-bf8d-88e79e5ce771",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"gcr.io/ml-pipeline/google-cloud-pipeline-components:latest\",\n",
    "           packages_to_install = [\"pandas\"],          \n",
    "          )\n",
    "def bq_load(\n",
    "    train_data: Output[Dataset]\n",
    ") -> str:\n",
    "    import pandas\n",
    "    from google.cloud import bigquery\n",
    "    client = bigquery.Client(location=\"US\", project='hackteam-mythbusters1')\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT * FROM `hackteam-mythbusters1.covid_dataset.combined1`\n",
    "    \"\"\"\n",
    "    query_job = client.query(\n",
    "        query,\n",
    "        location=\"US\",\n",
    "    )\n",
    "\n",
    "    df = query_job.to_dataframe()\n",
    "    df.to_csv(train_data.path, index=False)\n",
    "    return(train_data.path.replace(\"/gcs/\", \"gs://\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0be1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"gcr.io/ml-pipeline/google-cloud-pipeline-components:latest\",\n",
    "           packages_to_install = [\"pandas\",\"regex\"],          \n",
    "          )\n",
    "def preprocess_tweet(\n",
    "    dataset: Input[Dataset],\n",
    "    final: Output[Dataset]):\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    data = pd.read_csv(dataset.path)\n",
    "    data[\"Tweet_ID\"]=data[\"Tweet_ID\"].astype(str)\n",
    "    df_tweet = data[data['Translated_Tweet_Text'].isnull() == False]\n",
    "    #[['Tweet_ID','Translated_Tweet_Text']]\n",
    "    def preprocess(tweet):\n",
    "        processed_tweet = \"\"\n",
    "        processed_tweet = re.sub(r\"https\\S+\", \"\", tweet) #remove urls\n",
    "        processed_tweet = re.sub(\"[^a-zA-Z]+\", \" \", processed_tweet) # remove punctuations, digits, symbols\n",
    "        processed_tweet = re.sub(\"\\s{2,}\", \" \", processed_tweet) # Merged two+ spaces to one\n",
    "        return processed_tweet.strip()\n",
    "    df_tweet[\"Preprocessed_Tweet\"] = df_tweet[\"Translated_Tweet_Text\"].apply(preprocess)\n",
    "    #df_tweet = df_tweet.iloc[0:30,:]\n",
    "    df_tweet.to_csv(final.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6551f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"gcr.io/ml-pipeline/google-cloud-pipeline-components:latest\",\n",
    "           packages_to_install = [\"pandas\",\"regex\",\"transformers\",\"tensorflow\",\"torch\",\"flax\"],          \n",
    "          )\n",
    "def classify_tweet_train(\n",
    "    dataset: Input[Dataset],\n",
    "    final: Output[Dataset]):\n",
    "    \n",
    "    from transformers import pipeline\n",
    "    import pandas as pd\n",
    "    df_tweet = pd.read_csv(dataset.path)\n",
    "    classifier = pipeline(\"zero-shot-classification\",model=\"facebook/bart-large-mnli\")\n",
    "    def classify_tweet(tweet):\n",
    "        if((tweet is None) or (tweet=='')):\n",
    "            return None\n",
    "        else:\n",
    "            candidate_labels = ['health', 'business','technology','entertainment','sports','science']\n",
    "            model_result = classifier(tweet, candidate_labels)\n",
    "            return model_result\n",
    "    df_tweet[\"Class_General\"] = df_tweet[\"Preprocessed_Tweet\"].apply(classify_tweet)\n",
    "    label_li=[]\n",
    "    scores_li=[]\n",
    "    for index,row in df_tweet.iterrows():\n",
    "        model_res = row[\"Class_General\"]\n",
    "        if(model_res is not None):\n",
    "            label_li.append(model_res[\"labels\"])\n",
    "            scores_li.append(model_res[\"scores\"])\n",
    "        else:\n",
    "            label_li.append(None)\n",
    "            scores_li.append(None)\n",
    "    df_tweet[\"Class_Labels_Prioirity\"] = label_li\n",
    "    df_tweet[\"Class_Scores_Prioirity\"] = scores_li\n",
    "    df_tweet[\"Output_Class\"] = df_tweet[\"Class_Labels_Prioirity\"].apply(lambda x:x[0] if x is not None else None)\n",
    "    ## check 'health' in top 3 probabilities then classfiy as covid\n",
    "    df_tweet[\"Health_vs_Others\"] = df_tweet[\"Class_Labels_Prioirity\"].apply(lambda x:\"Health\" if ((x is not None) and ('health'in x[:-3])) else (\"Others\" if ((x is not None) and ('health'not in x[:-3])) else None ) )\n",
    "    df_tweet_health = df_tweet[df_tweet[\"Health_vs_Others\"] == \"Health\"]\n",
    "    df_tweet_health = df_tweet_health.iloc[:,0:20]\n",
    "    df_tweet_health.to_csv(final.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6250c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"gcr.io/ml-pipeline/google-cloud-pipeline-components:latest\",\n",
    "           packages_to_install = [\"pandas\",\"regex\",\"pytz\",\"numpy\",\"nltk\"],          \n",
    "          )\n",
    "def preprocess(\n",
    "    dataset: Input[Dataset],\n",
    "    final: Output[Dataset]):\n",
    "    # Functions\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import pytz\n",
    "    import numpy as np\n",
    "    from functools import reduce\n",
    "    import nltk\n",
    "    from nltk import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk import pos_tag\n",
    "    \n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    df = pd.read_csv(dataset.path)\n",
    "    \n",
    "    def preprocess_data(df):\n",
    "    \n",
    "        # For fetching the @mentions into new column\n",
    "        def get_mentions(s):\n",
    "            pattern = r'\\B(\\@[a-zA-Z0-9_-]+\\b)'\n",
    "            mentions = re.findall(pattern, s['Translated_Tweet_Text'])\n",
    "            return mentions\n",
    "\n",
    "        # For removing the mentions\n",
    "        def filter_mentions(s):\n",
    "            pattern = r'\\B(\\@[a-zA-Z0-9_-]+\\b)'\n",
    "            filtered = re.sub(pattern, '', s['Translated_Tweet_Text'])\n",
    "            return filtered\n",
    "\n",
    "        # For fetching the hashtags into new column\n",
    "        def get_hashtags(s):\n",
    "            pattern = r'\\B(\\#[a-zA-Z0-9_-]+\\b)'\n",
    "            hashtags = re.findall(pattern, s['Translated_Tweet_Text'])\n",
    "            return hashtags\n",
    "\n",
    "        # For removing the hashtags\n",
    "        def filter_hashtags(s):\n",
    "            pattern = r'\\B(\\#[a-zA-Z0-9_-]+\\b)'\n",
    "            filtered = re.sub(pattern, '', s['Translated_Tweet_Text'])\n",
    "            return filtered\n",
    "\n",
    "        # Calculating age of profile of user\n",
    "        def calculate_profile_age(s):\n",
    "            return (pd.Timestamp.now().tz_localize(tz=pytz.UTC) - \n",
    "                    s['Profile_Created_At'].tz_convert(tz=pytz.UTC)).total_seconds()\n",
    "\n",
    "        # For fetching the URLs in tweet body\n",
    "        def get_urls(s):\n",
    "            pattern = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "            urls = re.findall(pattern, s['Translated_Tweet_Text'])\n",
    "            return urls\n",
    "\n",
    "        # For removing the URLs\n",
    "        def filter_urls(s):\n",
    "            pattern = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "            filtered = re.sub(pattern, '', s['Translated_Tweet_Text'])\n",
    "            return filtered\n",
    "\n",
    "        def remove_punctuations(s):\n",
    "            pattern = r'[^a-zA-Z0-9 ]'\n",
    "            cleaned = re.sub(pattern,'',s['Translated_Tweet_Text'])\n",
    "            return cleaned\n",
    "\n",
    "        # For tokenizing the text\n",
    "        def tokenize(s):\n",
    "            tokens = word_tokenize(s['Translated_Tweet_Text'])\n",
    "            return tokens\n",
    "\n",
    "        # Stopwords being removed here\n",
    "        def remove_stopwords(s):\n",
    "            filtered = [x for x in s['Word_Tokens'] if not x in stop_words]\n",
    "            return filtered\n",
    "\n",
    "        # Performing lemmatization with POS tagging\n",
    "        def lemmatize(s):   \n",
    "\n",
    "            pos_tups = pos_tag(s['Word_Tokens'])\n",
    "\n",
    "            wordnet_pos = []\n",
    "            lemmatized = []\n",
    "\n",
    "            for tup in pos_tups:\n",
    "                if tup[1].startswith('J'):\n",
    "                    wordnet_pos.append((tup[0], wordnet.ADJ))\n",
    "                elif tup[1].startswith('V'):\n",
    "                    wordnet_pos.append((tup[0], wordnet.VERB))\n",
    "                elif tup[1].startswith('N'):\n",
    "                    wordnet_pos.append((tup[0], wordnet.NOUN))\n",
    "                elif tup[1].startswith('R'):\n",
    "                    wordnet_pos.append((tup[0], wordnet.ADV))\n",
    "                else:\n",
    "                    wordnet_pos.append((tup[0], None))\n",
    "\n",
    "            for w in wordnet_pos:\n",
    "                if w[1]:\n",
    "                    lemmatized.append(lemmatizer.lemmatize(w[0], pos=w[1]))\n",
    "                else:\n",
    "                    lemmatized.append(lemmatizer.lemmatize(w[0]))\n",
    "\n",
    "            return lemmatized\n",
    "\n",
    "        # Dropping Profile Location column because of too many NULLs (> 25%)\n",
    "        df = df.drop('Profile_Location', axis=1)\n",
    "\n",
    "        # Dropping rows based on nulls in the remaining columns\n",
    "        df = df[df['Followers_Count'].notna()]\n",
    "        df = df[df['Tweet_Text'].notna()]\n",
    "\n",
    "        # Encoding target variable \n",
    "        df['Profile_Verified'] = df['Profile_Verified'].apply(lambda x: 0 if x == False else 1)\n",
    "        df['Profile_Created_At'] = pd.to_datetime(df['Profile_Created_At'])\n",
    "\n",
    "        df['URLs'] = df.apply(get_urls, axis=1)\n",
    "        df['Translated_Tweet_Text'] = df.apply(filter_urls, axis=1)\n",
    "        df['Profile_Age'] = df.apply(calculate_profile_age, axis=1)\n",
    "        df['Hashtags'] =  df.apply(get_hashtags, axis=1)\n",
    "        df['Translated_Tweet_Text'] = df.apply(filter_hashtags, axis=1)\n",
    "        df['Mentions'] = df.apply(get_mentions, axis=1)\n",
    "        df['Translated_Tweet_Text'] = df.apply(filter_mentions, axis=1)\n",
    "\n",
    "        # Creating three derived variables based on count\n",
    "        df['Mentions_Count'] = df['Mentions'].apply(lambda x:len(x))\n",
    "        df['Hashtags_Count'] = df['Hashtags'].apply(lambda x: len(x))\n",
    "        df['URLs_Count'] = df['URLs'].apply(lambda x: len(x))\n",
    "\n",
    "        # Cleaning the tweet text column\n",
    "        df['Translated_Tweet_Text'] = df['Translated_Tweet_Text'].replace('\\n', ' ', regex=True)\n",
    "        df['Translated_Tweet_Text'] = df['Translated_Tweet_Text'].apply(lambda x: x.lower())\n",
    "        df['Translated_Tweet_Text'] = df.apply(remove_punctuations, axis=1)\n",
    "        df['Translated_Tweet_Text'] = df['Translated_Tweet_Text'].apply(lambda x: x.strip())\n",
    "        df['Word_Tokens'] = df.apply(tokenize, axis=1)\n",
    "        df['Word_Tokens'] = df.apply(remove_stopwords, axis=1)\n",
    "        df['Word_Tokens'] = df.apply(lemmatize, axis=1)\n",
    "        to_drop = ['Tweet_ID', 'Tweet_URL', 'Tweet_Text', 'Retweet_Count', 'Reply_Count', 'Quote_Count', 'Like_Count', \n",
    "                   'Tweet_Created_At', 'Tweet_Handles', 'Profile_Created_At', 'Profile_Image_URL', 'Translated_Tweet_Text',\n",
    "                  'Hashtags', 'Mentions', 'URLs']\n",
    "\n",
    "        df_filt = df.drop(to_drop, axis=1)\n",
    "        \n",
    "        return df_filt\n",
    "\n",
    "    df_filt = preprocess_data(df)\n",
    "    assert len(df_filt.columns) == 12\n",
    "    \n",
    "    #Final Write\n",
    "    df_filt.to_csv(final.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"gcr.io/ml-pipeline/google-cloud-pipeline-components:latest\",\n",
    "           packages_to_install = [\"pandas\",\"scikit-learn\", \"numpy\"],          \n",
    "          )\n",
    "def train_component(\n",
    "    dataset: Input[Dataset],\n",
    "    accuracy: Output[Metrics],\n",
    "    f1score: Output[Metrics],\n",
    "    metricsc: Output[ClassificationMetrics],\n",
    "    model : Output[Model],\n",
    "    encoder: Output[Model],\n",
    "    tfidfvec: Output[Model],\n",
    "    standardscaler: Output[Model]\n",
    ")-> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"model_path\", str),\n",
    "        (\"encoder_path\", str),\n",
    "        (\"standard_scaler_path\", str),\n",
    "        (\"tfidf_vec_path\", str),\n",
    "    ],\n",
    "):\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import os\n",
    "    from google.cloud import aiplatform, storage\n",
    "    from sklearn.metrics import  accuracy_score, confusion_matrix, f1_score, log_loss, roc_curve\n",
    "    \n",
    "    df_filt = pd.read_csv(dataset.path)\n",
    "    \n",
    "    def train_model(df_filt):\n",
    "        # Randomizing the dataset\n",
    "        df_filt = df_filt.sample(frac=1)\n",
    "\n",
    "        # Performing one hot encoding on Tweet Source variable\n",
    "        one_hot_enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        source_onehot = one_hot_enc.fit_transform(df_filt[['Tweet_Source']])\n",
    "        df_filt[one_hot_enc.categories_[0]] = source_onehot.toarray().astype(int)\n",
    "\n",
    "        # Fitting standardscaler on the numerical columns, this scaler needs to be saved for transforming during runtime\n",
    "        scaler = StandardScaler()\n",
    "        df_filt[['Followers_Count', 'Following_Count', 'Tweet_Count', 'Listed_Count', 'Profile_Age', 'Mentions_Count', 'Hashtags_Count', 'URLs_Count']] = scaler.fit_transform(df_filt[['Followers_Count', 'Following_Count', 'Tweet_Count', 'Listed_Count', 'Profile_Age', 'Mentions_Count', 'Hashtags_Count', 'URLs_Count']])\n",
    "\n",
    "        # Fitting tf-idf vectorizer on the word tokens, this vectorizer needs to be saved for transforming during runtime\n",
    "        v = TfidfVectorizer()\n",
    "        df_filt['Word_Tokens'] = df_filt['Word_Tokens'].apply(lambda x:''.join(x))\n",
    "        text_vectors = v.fit_transform(df_filt['Word_Tokens']).todense().tolist()\n",
    "        rr = pd.DataFrame(text_vectors)\n",
    "        df_filt = df_filt.reset_index()\n",
    "        df_final = df_filt.merge(rr, left_index=True, right_index=True)\n",
    "\n",
    "        to_drop_final = ['index', 'Word_Tokens', 'Tweet_Source']\n",
    "        df_final = df_final.drop(to_drop_final, axis=1)\n",
    "\n",
    "        # Separating feature and target variables\n",
    "        Y = df_final['label']\n",
    "        X = df_final.drop(['label'], axis=1)\n",
    "\n",
    "        # 70-30 split between train and test datasets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "        # Defining model and training the same\n",
    "        model_gbm = GradientBoostingClassifier()\n",
    "        model_gbm.fit(X_train, y_train)\n",
    "        predictions = model_gbm.predict(X_test)\n",
    "        \n",
    "        predictions = predictions.reshape(-1,1)\n",
    "        y_test = np.array(y_test).reshape(-1,1)\n",
    "        \n",
    "        # accuracy on test set\n",
    "        Classifier_Accuracy = accuracy_score(y_test,predictions)\n",
    "        accuracy.log_metric(\"accuracy\", (Classifier_Accuracy * 100.0))\n",
    "\n",
    "        # F1 \n",
    "        F1_Score = f1_score(y_test,predictions)\n",
    "        f1score.log_metric(\"F1 Score\", (F1_Score*100.0))\n",
    "        \n",
    "        metricsc.log_confusion_matrix(\n",
    "        [\"Genuine\", \"Fake\"],\n",
    "        confusion_matrix(\n",
    "            y_test, predictions\n",
    "            ).tolist(),  # .tolist() to convert np array to list.\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        # Saving the encoder, scaler, vectorizer and model to pickles    \n",
    "    #     with open('one_hot_enc.pkl', 'wb') as handle:\n",
    "    #         pickle.dump(one_hot_enc, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    #     with open('scaler.pkl', 'wb') as handle:\n",
    "    #         pickle.dump(scaler, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    #     with open('tfidf_vectorizer.pkl', 'wb') as handle:\n",
    "    #         pickle.dump(v, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    #     with open('model_gbm.pkl', 'wb') as handle:\n",
    "    #         pickle.dump(model_gbm, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Printing classification report\n",
    "        # print(classification_report(y_test, predictions))\n",
    "        return(model_gbm,one_hot_enc,scaler,v)\n",
    "    \n",
    "    def save_model_to_bucket(artifact_filename,Output_Model,Model):\n",
    "        local_path = artifact_filename\n",
    "        pickle.dump(Model, open(artifact_filename, \"wb\"))\n",
    "        # Upload model artifact to Cloud Storage\n",
    "        model_directory = Output_Model.path\n",
    "        model_directory_gs = model_directory.replace(\"/gcs/\", \"gs://\")\n",
    "        storage_path = os.path.join(model_directory_gs, artifact_filename)\n",
    "        blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())\n",
    "        blob.upload_from_filename(local_path)\n",
    "    \n",
    "    model_gbm,one_hot_enc,scaler,v = train_model(df_filt)\n",
    "    \n",
    "    save_model_to_bucket('model.pkl',model,model_gbm)\n",
    "    save_model_to_bucket('model.pkl',encoder,one_hot_enc)\n",
    "    save_model_to_bucket('model.pkl',standardscaler,scaler)\n",
    "    save_model_to_bucket('model.pkl',tfidfvec,v)\n",
    "    return(model.path.replace(\"/gcs/\", \"gs://\"),\n",
    "           encoder.path.replace(\"/gcs/\", \"gs://\"),\n",
    "           standardscaler.path.replace(\"/gcs/\", \"gs://\"),\n",
    "           tfidfvec.path.replace(\"/gcs/\", \"gs://\"),\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad1ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"gcr.io/ml-pipeline/google-cloud-pipeline-components:latest\", packages_to_install=['google-cloud-aiplatform'],)\n",
    "def import_model(\n",
    "    project_id: str,\n",
    "    display_name: str,\n",
    "    artifact_gcs_bucket: str,\n",
    "    model: Output[Model],\n",
    "    location: str,\n",
    "    serving_container_image_uri: str,\n",
    "    description: str\n",
    ") -> NamedTuple(\n",
    "    'Outputs', \n",
    "    [ \n",
    "        ('display_name', str), \n",
    "        ('resource_name', str)\n",
    "    ]\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project_id, location=location)\n",
    "    model_resp = aiplatform.Model.upload(\n",
    "        display_name=display_name,\n",
    "        artifact_uri=artifact_gcs_bucket,\n",
    "        serving_container_image_uri=serving_container_image_uri,\n",
    "        description=description)\n",
    "    model_resp.wait()\n",
    "    with open(model.path, 'w') as f: \n",
    "      f.write(model_resp.resource_name)\n",
    "    model.path = f\"aiplatform://v1/{model_resp.resource_name}\" #update the resource path to aiplaform://v1 prefix so that off the shelf tasks can consume the output\n",
    "    return (model_resp.display_name, model_resp.resource_name,)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2de0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=PIPELINE_EXPERIMENT_NAME,\n",
    ")\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    region:str = REGION,\n",
    "):\n",
    "    dataset_op = bq_load()\n",
    "    dataset_filtered = preprocess_tweet(dataset_op.outputs['train_data'])\n",
    "    #classify_tweets = classify_tweet_train(dataset_filtered.outputs['final'])\n",
    "    processed_data = preprocess(dataset_op.outputs[\"train_data\"])\n",
    "    train_val = train_component(processed_data.outputs[\"final\"])\n",
    "    \n",
    "    model_upload_op_1 = import_model(\n",
    "        project_id=project,\n",
    "        display_name='main-pipeline-scoring-2',\n",
    "        serving_container_image_uri=\"europe-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    "        artifact_gcs_bucket=train_val.outputs[\"model_path\"] ,\n",
    "        location=REGION,\n",
    "        description=\"final model\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "#     create_endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "#         project=project,\n",
    "#         display_name = \"main-scoring-model\",\n",
    "#         location = region\n",
    "#     )\n",
    "\n",
    "#     model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "#         model=model_upload_op_1.outputs[\"model\"],\n",
    "#         endpoint=create_endpoint_op.outputs['endpoint'],\n",
    "#         automatic_resources_min_replica_count=1,\n",
    "#         automatic_resources_max_replica_count=1,\n",
    "#     )\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  # noqa: F811\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=PIPELINE_JSON_FILE\n",
    ")\n",
    "\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=PIPELINE_EXPERIMENT_NAME,\n",
    "    template_path=PIPELINE_JSON_FILE,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\"project\":PROJECT_ID,\"region\":REGION},\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ad0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient  # noqa: F811\n",
    "\n",
    "api_client = AIPlatformClient(project_id=PROJECT_ID, region=REGION)\n",
    "\n",
    "# adjust time zone and cron schedule as necessary\n",
    "response = api_client.create_schedule_from_job_spec(\n",
    "    job_spec_path=PIPELINE_JSON_FILE,\n",
    "    schedule=\"0 6 1 * *\",\n",
    "    time_zone=\"America/Los_Angeles\",  # change this as necessary\n",
    "    pipeline_root=PIPELINE_ROOT  # this argument is necessary if you did not specify PIPELINE_ROOT as part of the pipeline definition.\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
